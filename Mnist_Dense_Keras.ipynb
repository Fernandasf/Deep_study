{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Full-Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prologue\n",
    "\n",
    "The classification problem was solved using a kind of Neural Network, which contains just Full-Connected Layers (named like Dense too). We used differents configurations and parameters to obtain the best result. Moreover, We used MNIST dataset which is provided by Keras.\n",
    "\n",
    "\n",
    "Fixed parameters:\n",
    "\n",
    "    - batch size = 128\n",
    "    - epochs = 12\n",
    "    - loss function = categorical crossentropy\n",
    "    - optimizer = stocastic gradient descedent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernanda/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (60000, 28, 28)\n",
      "y_train shape (60000,)\n",
      "X_test shape (10000, 28, 28)\n",
      "y_test shape (10000,)\n",
      "Train matrix shape (60000, 784)\n",
      "Test matrix shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# let's print the shape before we reshape and normalize\n",
    "print(\"X_train shape\", x_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", x_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)\n",
    "\n",
    "# building the input vector from the 28x28 pixels\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# normalizing the data to help with the training\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# print the final input shape ready for training\n",
    "print(\"Train matrix shape\", x_train.shape)\n",
    "print(\"Test matrix shape\", x_test.shape)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#print (y_test.shape)\n",
    "#print ('input_shape:', input_shape)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Dense + ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"I1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(Flatten())\n",
    "\n",
    "# Input Layer\n",
    "input_shape=784\n",
    "# Hidden Layer\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "# Output Layer\n",
    "model.add(Dense(num_classes, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 3.9605 - acc: 0.1022 - val_loss: 3.6637 - val_acc: 0.1010\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 3.5389 - acc: 0.1023 - val_loss: 3.5300 - val_acc: 0.1012\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 3.4087 - acc: 0.1063 - val_loss: 3.3889 - val_acc: 0.1111\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 3.2229 - acc: 0.1685 - val_loss: 3.1190 - val_acc: 0.2811\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 1.9691 - acc: 0.4330 - val_loss: 1.1703 - val_acc: 0.6023\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 1.4491 - acc: 0.3497 - val_loss: 1.0918 - val_acc: 0.5322\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 1.0499 - acc: 0.6184 - val_loss: 0.8490 - val_acc: 0.7494\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.9935 - acc: 0.6902 - val_loss: 0.9121 - val_acc: 0.7129\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 1.5352 - acc: 0.2914 - val_loss: 1.6605 - val_acc: 0.1187\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 1.4486 - acc: 0.2661 - val_loss: 1.1872 - val_acc: 0.5086\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.8863 - acc: 0.7424 - val_loss: 0.6925 - val_acc: 0.8434\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.6767 - acc: 0.8664 - val_loss: 0.6509 - val_acc: 0.8900\n",
      "Test loss: 0.6509352695226669\n",
      "Test accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "#model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='sgd')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Dense + ReLu + Dense + Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"I1_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "input_shape=784\n",
    "# Hidden Layer\n",
    "model2.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "# Output Layer\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "           \n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 1.0770 - acc: 0.7703 - val_loss: 0.5884 - val_acc: 0.8685\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 0.5166 - acc: 0.8726 - val_loss: 0.4296 - val_acc: 0.8922\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.4188 - acc: 0.8892 - val_loss: 0.3701 - val_acc: 0.9022\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3734 - acc: 0.8985 - val_loss: 0.3381 - val_acc: 0.9081\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.3454 - acc: 0.9049 - val_loss: 0.3172 - val_acc: 0.9147\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3252 - acc: 0.9098 - val_loss: 0.3011 - val_acc: 0.9180\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3094 - acc: 0.9141 - val_loss: 0.2878 - val_acc: 0.9207\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2964 - acc: 0.9180 - val_loss: 0.2775 - val_acc: 0.9221\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.2854 - acc: 0.9204 - val_loss: 0.2683 - val_acc: 0.9248\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2755 - acc: 0.9232 - val_loss: 0.2600 - val_acc: 0.9278\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2669 - acc: 0.9256 - val_loss: 0.2525 - val_acc: 0.9299\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2589 - acc: 0.9281 - val_loss: 0.2456 - val_acc: 0.9312\n",
      "Test loss: 0.24561540707051754\n",
      "Test accuracy: 0.9312\n"
     ]
    }
   ],
   "source": [
    "#model2.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model2.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='sgd')\n",
    "\n",
    "model2.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We could notice that the accuracy increased from 0.89 to 0.9312 when using the softmax activation function. It happened because this function normalizes the vector into a probability distribution. In other words, this function normalize the outputs in the interval between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://nextjournal.com/gkoehler/digit-recognition-with-keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
